#summary Documentation for using mpmath
#labels Featured

This page contains an introduction to mpmath. It does not cover all features. Most mpmath modules contain a fair amount of comments and docstrings, however, so it should be fairly easy to figure out how to use something by looking at the source code, or by looking at the [http://mpmath.googlecode.com/svn/trunk/tests/ unit tests].

==Installation==

The latest version can be download from the [http://code.google.com/p/mpmath/downloads/list download page] or the links on the front page. To install mpmath, either run the binary installer (Windows only) or unzip the source package and run `python setup.py install`.

If you download the source package, some extra test files are included. You can test that mpmath works as it should by running `python runtests.py` in the tests directory (requires [http://codespeak.net/py/dist/test.html py.test]). There is also an interactive demo script `pidigits.py` for computing (lots of) digits of pi.

==Basic usage==

To import mpmath in an interactive Python session, type

{{{
>>> from mpmath import *
}}}

mpmath provides two classes `mpf` and `mpc` which respectively correspond to Python's built-in `float` and `complex`. They interact nicely with other numbers:

{{{
>>> mpf(3) + 2*mpf('2.5') + 1.0
mpf('9')
>>> mpc(1j)**0.5
mpc(real='0.70710678118654757', imag='0.70710678118654757')
}}}

The `print` statement displays numbers more prettily:
{{{
>>> print mpc(1j)**0.5
(0.707106781186548 + 0.707106781186548j)
}}}

==Managing precision==

As seen above, mpfs have a default precision of roughly 15 decimal digits (more precisely 53 bits), the same as standard floats. To change the precision, use the `prec` or `dps` properties of the `mpf` class. `prec` is the precision in bits and `dps` is the precision in decimals; if you change one, the `mpf` class automatically changes the other to match. For example, this sets the precision to 100 digits:

{{{
>>> mpf.dps = 100
>>> mpf.dps
100
>>> mpf.prec
336
}}}

This increases the precision by two bits and then restores it:
{{{
>>> mpf.prec += 2
>>> mpf.prec -= 2
}}}

When you've set the precision level, all mpf and mpc operations are carried out at that precision:

{{{
>>> mpf.dps = 50
>>> mpf(1) / 7
mpf('0.1428571428571428571428571428571428571428571428571428')
>>> mpf.dps = 100
>>> mpf(2) ** 0.5
mpf('1.4142135623730950488016887242096980785696718753769480731766797379907324784
6210703885038753432764157274')
}}}

Note that when creating a new mpf, the value will at most be as accurate as the input. In particular, watch out for creating mpfs from Python floats, which generally have only 15-digit accuracy. When working with high precision, initialize fractional mpf values from strings or integers:
{{{
>>> mpf.dps = 30
>>> mpf(10.9)   # bad
mpf('10.90000000000000035527136788005')
>>> mpf('10.9')  # good
mpf('10.9')
>>> mpf(109) / mpf(10)   # also good
mpf('10.9')
}}}

As with Python floats, mpfs can not generally represent decimal fractions exactly, although the error can be made arbitrarily small. Even with dps=30, the representation of 10.9 actually has an error of about 3 × 10^-31^, and these errors propagate across calculations. If you need a certain accuracy in the final result of a calculation, you should temporarily increase the working precision by a couple of digits (indeed, the ability to do so is the main feature of mpmath!). For more information about issues with precision of binary floating-point numbers, read [http://docs.sun.com/source/806-3568/ncg_goldberg.html What Every Computer Scientist Should Know About Floating-Point Arithmetic].

There is no practical magnitude limitation on numbers. For example, an mpf can represent an approximation of 1 googolplex:

{{{
>>> a = mpf(10) ** (10**100)
>>> print a
1.0e+100000000000000000000000000000000000000000000000000000000000000000000000000
00000000000000000000000000
>>> print log(a, 10)
1.0e+100
}}}

The objects `pi`, `e`, and `euler` (Euler's constant) are special objects that behave like mpfs but whose values automatically adjust to the precision.

{{{
>>> mpf.dps = 15
>>> print pi
3.14159265358979
>>> mpf.dps = 50
>>> print pi
3.1415926535897932384626433832795028841971693993751
>>> print e
2.7182818284590452353602874713526624977572470937
>>> print euler
0.57721566490153286060651209008240243104215933593992
}}}

==Rounding==

Directed rounding is partially implemented. For example, this computes and verifies a 15-digit approximation interval for pi:

{{{
>>> mpf.dps = 15
>>> mpf.round_down(); pi1 = +pi
>>> mpf.round_up(); pi2 = +pi
>>> pi1
mpf('3.1415926535897931')
>>> pi2
mpf('3.1415926535897936')
>>> mpf.dps = 30
>>> pi1 < pi < pi2
True
}}}

Correct rounding does not yet work for all operations, however.

==Special functions==
mpmath implements all the standard math functions, for both real and complex numbers and with arbitrary precision:

{{{
>>> mpf.dps = 15
>>> cosh(1.234)
mpf('1.8630338016984225')
>>> asin(1.0)
mpf('1.5707963267948966')
>>> log(1+2j)
mpc(real='0.80471895621705025', imag='1.1071487177940904')

>>> mpf.dps = 25
>>> exp(2+3j)
mpc(real='-7.31511009490110251748653617', imag='1.04274365623590441410150394')
}}}

Some functions that do not exist in the standard Python math library are available, e.g. factorials (with support for noninteger arguments, of course):

{{{
>>> mpf.dps = 15
>>> factorial(10)
mpf('3628800.0')
>>> factorial(0.25)
mpf('0.90640247705547705')
>>> factorial(2+3j)
mpc(real='-0.44011340763700169', imag='-0.063637243126317022')
}}}

==Table of available functions==

|| *Function* || *Description* ||
|| sqrt(x) || Square root ||
|| hypot(x,y) || Euclidean distance ||
|| exp(x) || Exponential function ||
|| log(x,b) || Natural logarithm (optionally base-b logarithm) ||
|| power(x,y) || Power, x^y ||
|| cos(x) || Cosine ||
|| sin(x) || Sine ||
|| tan(x) || Tangent ||
|| cosh(x) || Hyperbolic cosine ||
|| sinh(x) || Hyperbolic sine ||
|| tanh(x) || Hyperbolic tangent ||
|| acos(x) || Inverse cosine ||
|| asin(x) || Inverse sine ||
|| atan(x) || Inverse tangent ||
|| acosh(x) || Inverse hyperbolic cosine ||
|| asinh(x) || Inverse hyperbolic sine ||
|| atanh(x) || Inverse hyperbolic tangent ||
|| floor(x) || Floor function (round to integer in the direction of -inf) ||
|| ceil(x) || Ceiling function (round to integer in the direction of +inf) ||
|| arg(x) || Complex argument ||
|| rand() || Generate a random number in [0, 1) ||
|| factorial(x) || Factorial ||
|| gamma(x) || Gamma function ||
|| lower_gamma(a,x) || Lower gamma function ||
|| upper_gamma(a,x) || Upper gamma function ||
|| erf(x) || Error function ||
|| zeta(x) || Riemann zeta function ||

Special numbers:
|| *Symbol* || *Description* ||
|| j || Imaginary unit ||
|| pi || Circle constant pi ~= 3.14159 ||
|| e || Euler's number (base of the natural logarithm) e ~= 2.71828 ||
|| euler || Euler's constant gamma ~= 0.577216 ||
|| clog2 || log(2) ~= 0.693147 ||
|| clog10 || log(10) ~= 2.30259 ||
|| inf || Positive infinity ||
|| -inf || Negative infinify ||
|| nan || Not-a-number ||


==Internal number representation==
A binary floating-point number is an exact rational number of the form `x` = `man` × 2^exp^ where `man` and `exp` are integers (called the _mantissa_ and _exponent_). In `mpmath`, a floating-point number is represented as a three-element tuple

    `(man, exp, bc)`

where `bc` is the number of bits in the mantissa. (The `bc` element is strictly speaking redundant since one can re-count the number of bits in the mantissa at any time, but the bitcounts are frequently needed for internal computations and storing them significantly improves performance.) There is one additional requirement for the representation: `man` must have no trailing zero bits (i.e., all powers of two must be factored out and stored in `exp`). This normalization ensures that two floating-point numbers are mathematically equal if and only if their tuple representations are equal, meaning that one can check the equality of two numbers by a simple tuple comparison.

Some examples of floating-point numbers in this format are:

    `1   = (1, 0, 1)`

    `2   = (1, 1, 1)`

    `10  = (5, 1, 3)`

    `0.5 = (1, -1, 1)`

By convention, 0 is taken to have the representation `(0, 0, 0)`.

The modules in the `mpmath.lib` directory contain functions for working directly with tuples. In general, these functions accept as input one or two operands, followed by an option `prec` that specifies the precision and `rounding` that specifies the direction of rounding. For example, to add the numbers 2 and 10, we can use the `fadd` function:

{{{
>>> from mpmath.lib import *
>>> two = (1, 1, 1)
>>> ten = (5, 1, 3)
>>> twelve = fadd(two, ten, 10, ROUND_FLOOR)
>>> twelve
(3, 2, 2)
}}}

Of course, since this sum can be represented exactly with only 10 bits in the mantissa, no rounding was performed.